<!DOCTYPE html>
<html>
<head>
    <title>MedVidQA 2024</title>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width">

    <!-- style -->
    <link rel="stylesheet" type="text/css" href="css/style.css">
    <link rel="stylesheet" type="text/css" href="css/datatables.min.css">
    <link rel="stylesheet" type="text/css" href="css/magnific-popup.css">
    <link rel="stylesheet" type="text/css" href="https://cdn.datatables.net/v/bs4-4.1.1/jszip-2.5.0/dt-1.10.20/b-1.6.1/b-html5-1.6.1/b-print-1.6.1/cr-1.5.2/r-2.2.3/datatables.min.css"/>

    <!-- fonts -->
    <link href="https://fonts.googleapis.com/css?family=Roboto&display=swap" rel="stylesheet">

    <!-- javascript libraries -->
    <script src="https://www.gstatic.com/charts/loader.js"></script>
    <script type="text/javascript" src="js/jquery.latest.min.js"></script>
    <script type="text/javascript" src="js/jquery.csv.min.js"></script>
    <script type="text/javascript" src="js/jquery.magnific-popup.min.js"></script>
    <script type="text/javascript" src="js/datatables.min.js"></script>

    <!-- datatables deps -->
    <script type="text/javascript" src="https://cdn.datatables.net/v/bs4-4.1.1/jszip-2.5.0/dt-1.10.20/b-1.6.1/b-html5-1.6.1/b-print-1.6.1/cr-1.5.2/r-2.2.3/datatables.min.js"></script>

    <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-30FQTDJR3S"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-30FQTDJR3S');
</script>
    <!-- Simple js to show/hide a div element -->
    <script>
        function myFunction(el) {
            var x = document.getElementById(el);
            if (x.style.display === "none") {
                x.style.display = "block";
            } else {
                x.style.display = "none";
            }
        }
    </script>


    
</head>

<body id="_index">

<nav>
<section style="text-align: right;">
    <span class="home">
        <a href="index.html"><strong>Home</strong></a>
        <a href="#idates"><strong>Important Dates</strong></a>
        <a href="#tasks"><strong>Tasks</strong></a>
        <a href="#datasets"><strong>Datasets</strong></a>
        <a href="#eval"><strong>Evaluation</strong></a>
  <a href="#organizers"><strong>Organizers</strong></a>
    </span>
</section>
</nav>

    <div style="height: 325px; width: 100%; overflow:hidden;   position: relative; text-align: center;  font-size: 180%;  
  color: Black;">
        <img src="img/background.png" style="width: 100%; height: 325px;"/>
         <div class="centered">
         <h1 style="font-size:2vw">TREC Task on Medical Video Question Answering</h1>
        <h1 style="font-size:3vw">MedVidQA 2024</h1>

     </div>
    </div>



<main>
    <section style="margin-top: 1cm">
<h2>Introduction</h2>


<div>
<p>
   The recent surge in the availability of online videos has changed the way of acquiring information and knowledge. Many people prefer instructional videos to teach or learn how to accomplish a particular task in an effective and efficient manner with a series of step-by-step procedures. Similarly, medical instructional videos are more suitable and beneficial for delivering key information through visual and verbal communication to consumers' healthcare questions that demand instruction. We aim to extract the visual information from the video corpus for consumers' first aid, medical emergency, and medical educational questions. Extracting the relevant information from the video corpus requires relevant video retrieval, moment localization, video summarization, and captioning skills. Toward this, the TREC task, Medical Video Question Answering, focuses on developing systems capable of understanding medical videos and providing visual answers (from single and multiple videos) and instructional step captions to answer natural language questions. Emphasizing the importance of multimodal capabilities, the task requires systems to generate instructional questions and captions based on medical video content. Following the <a href="./index-2023.html">MedVidQA 2023</a>, TREC 2024 expanded the tasks considering language-video understanding and generation. This track is comprised of two main tasks: Video Corpus Visual Answer Localization (VCVAL) and Query-Focused Instructional Step Captioning (QFISC).

</p>

</div>



</section>
<section id="news" style="margin-bottom: 1cm;">
<h2>News</h2>
<ul>


<li>February 12, 2024: Introducing the MedVidQA 2024 challenge.</li>


</ul>
</section>

<section id="idates" style="margin-bottom: 1cm;">
<h2>Important Dates</h2>
<ul>

<style type="text/css">
.tg  {border-collapse:collapse;border-spacing:0;}
.tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
  overflow:hidden;padding:10px 5px;word-break:normal;}
.tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
  font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}
.tg .tg-1wig{font-weight:bold;text-align:left;vertical-align:top}
.tg .tg-baqh{text-align:center;vertical-align:top}
.tg .tg-c3ow{border-color:inherit;text-align:center;vertical-align:top}
.tg .tg-7btt{border-color:inherit;font-weight:bold;text-align:center;vertical-align:top}
.tg .tg-amwm{font-weight:bold;text-align:center;vertical-align:top}
.tg .tg-rmlg{background-color:#FFF;border-color:inherit;color:#212529;text-align:left;vertical-align:top}
.tg .tg-mfw7{background-color:#FFF;color:#212529;text-align:left;vertical-align:top}
.tg-sort-header::-moz-selection{background:0 0}
.tg-sort-header::selection{background:0 0}.tg-sort-header{cursor:pointer}
.tg-sort-header:after{content:'';float:right;margin-top:7px;border-width:0 5px 5px;border-style:solid;
  border-color:#404040 transparent;visibility:hidden}
.tg-sort-header:hover:after{visibility:visible}
.tg-sort-asc:after,.tg-sort-asc:hover:after,.tg-sort-desc:after{visibility:visible;opacity:.4}
.tg-sort-desc:after{border-bottom:none;border-width:5px 5px 0}@media screen and (max-width: 767px) {.tg {width: auto !important;}.tg col {width: auto !important;}.tg-wrap {overflow-x: auto;-webkit-overflow-scrolling: touch;}}</style>
<div class="tg-wrap"><table id="tg-ErTjd" class="tg">
<thead>
  <tr>
    <th class="tg-1wig"></th>
    <th class="tg-7btt">Video Corpus<br>Release</th>
    <th class="tg-7btt">Training/Val Set<br>Release</th>
    <th class="tg-7btt">Test Set <br>Release</th>
    <th class="tg-amwm">Submission<br>Deadline</th>
    <th class="tg-amwm">Official<br>Results</th>
  </tr>
</thead>
<tbody>
  <tr>
    <td class="tg-1wig">Task A</td>
    <td class="tg-c3ow">May 12</td>
    <td class="tg-c3ow">April 30</td>
    <td class="tg-c3ow">June 7</td>
    <td class="tg-baqh">August 2</td>
    <td class="tg-baqh">September 8</td>
  </tr>
  <tr>
    <td class="tg-1wig">Task B</td>
    <td class="tg-c3ow">NA</td>
    <td class="tg-c3ow">April 30</td>
    <td class="tg-rmlg">September 2</td>
    <td class="tg-mfw7">September 16</td>
    <td class="tg-baqh">October 11</td>
  </tr>
</tbody>
</table></div>
<script charset="utf-8">var TGSort=window.TGSort||function(n){"use strict";function r(n){return n?n.length:0}function t(n,t,e,o=0){for(e=r(n);o<e;++o)t(n[o],o)}function e(n){return n.split("").reverse().join("")}function o(n){var e=n[0];return t(n,function(n){for(;!n.startsWith(e);)e=e.substring(0,r(e)-1)}),r(e)}function u(n,r,e=[]){return t(n,function(n){r(n)&&e.push(n)}),e}var a=parseFloat;function i(n,r){return function(t){var e="";return t.replace(n,function(n,t,o){return e=t.replace(r,"")+"."+(o||"").substring(1)}),a(e)}}var s=i(/^(?:\s*)([+-]?(?:\d+)(?:,\d{3})*)(\.\d*)?$/g,/,/g),c=i(/^(?:\s*)([+-]?(?:\d+)(?:\.\d{3})*)(,\d*)?$/g,/\./g);function f(n){var t=a(n);return!isNaN(t)&&r(""+t)+1>=r(n)?t:NaN}function d(n){var e=[],o=n;return t([f,s,c],function(u){var a=[],i=[];t(n,function(n,r){r=u(n),a.push(r),r||i.push(n)}),r(i)<r(o)&&(o=i,e=a)}),r(u(o,function(n){return n==o[0]}))==r(o)?e:[]}function v(n){if("TABLE"==n.nodeName){for(var a=function(r){var e,o,u=[],a=[];return function n(r,e){e(r),t(r.childNodes,function(r){n(r,e)})}(n,function(n){"TR"==(o=n.nodeName)?(e=[],u.push(e),a.push(n)):"TD"!=o&&"TH"!=o||e.push(n)}),[u,a]}(),i=a[0],s=a[1],c=r(i),f=c>1&&r(i[0])<r(i[1])?1:0,v=f+1,p=i[f],h=r(p),l=[],g=[],N=[],m=v;m<c;++m){for(var T=0;T<h;++T){r(g)<h&&g.push([]);var C=i[m][T],L=C.textContent||C.innerText||"";g[T].push(L.trim())}N.push(m-v)}t(p,function(n,t){l[t]=0;var a=n.classList;a.add("tg-sort-header"),n.addEventListener("click",function(){var n=l[t];!function(){for(var n=0;n<h;++n){var r=p[n].classList;r.remove("tg-sort-asc"),r.remove("tg-sort-desc"),l[n]=0}}(),(n=1==n?-1:+!n)&&a.add(n>0?"tg-sort-asc":"tg-sort-desc"),l[t]=n;var i,f=g[t],m=function(r,t){return n*f[r].localeCompare(f[t])||n*(r-t)},T=function(n){var t=d(n);if(!r(t)){var u=o(n),a=o(n.map(e));t=d(n.map(function(n){return n.substring(u,r(n)-a)}))}return t}(f);(r(T)||r(T=r(u(i=f.map(Date.parse),isNaN))?[]:i))&&(m=function(r,t){var e=T[r],o=T[t],u=isNaN(e),a=isNaN(o);return u&&a?0:u?-n:a?n:e>o?n:e<o?-n:n*(r-t)});var C,L=N.slice();L.sort(m);for(var E=v;E<c;++E)(C=s[E].parentNode).removeChild(s[E]);for(E=v;E<c;++E)C.appendChild(s[v+L[E-v]])})})}}n.addEventListener("DOMContentLoaded",function(){for(var t=n.getElementsByClassName("tg"),e=0;e<r(t);++e)try{v(t[e])}catch(n){}})}(document)</script>

</ul>
Join our <a href="https://groups.google.com/g/trec-medvidqa2024">Google Group</a> for important updates! If you have any questions, ask in our <a href="https://groups.google.com/g/trec-medvidqa2024">Google Group</a> or <a href="mailto:deepak.gupta@nih.gov">email</a> us.

</section>


<section id="registration" style="margin-bottom: 1cm;">
<h2>Registration and Submission</h2>

<ul>
<li> Participants are required to complete their registration by submitting the TREC 2024 Registration <a href="https://ir.nist.gov/evalbase/accounts/login/?next=/evalbase/">Form</a>. Registered teams will be added to TREC 2024's mailing list for future communication.
</li>

<li> Submission: TBD</li>

</ul>
</section>


<section id="tasks" style="margin-bottom: 1cm;">
<h2>Tasks</h2>
<ul>
	<li>
<h3>Task A: Video Corpus Visual Answer Localization (VCVAL)</h3>
Given a medical query and a collection of videos, the task aims to retrieve the appropriate video from the video collection and then locate the temporal segments (start and end timestamps) in the video where the answer to the medical query is being shown, or the explanation is illustrated in the video.

<br><br>
 <img src="img/vcval.PNG" style="width: 100%;object-fit: cover;"/>
  <div>
<center><h5><br>Schematic workflow of the video corpus visual answer localization task. </h5></center>
</div>
</li>
<div>
The VCVAL task consists of two subtasks: <b>(a)</b> Video Retrieval, and <b>(b)</b> Temporal Segment Prediction.
</div>
<br><br>

<li>
<h3>Task B: Query-Focused Instructional Step Captioning (QFISC)</h3>
Given a medical query and a video, this task aims to generate step-by-step textual summaries of the visual instructional segment that can be considered as the answer to the medical query. The proposed QFISC task can be considered an extension of the visual answer localization task, where the system needs to locate a series of instructional segments that serve as the answer to the query. The QFISC demands identifying the instructional step boundaries and generating a step caption for every step. This task comes under multimodal generation, where the system has to consider the video (visual) and subtitle (language) modality to generate the natural language caption.
</li>


</ul>
</section>


<section id="datasets" style="margin-bottom: 1cm;">
<h2>Datasets</h2>
<ul>
<li>
<h5>Task A</h5>
<ul>
<li>
    <strong>Training and Validation Datasets:</strong> 
    
    		MedVidQA collections <a href="#ref1">[1]</a> consisting of 3,010 human-annotated instructional questions and visual answers from 900 health-related videos.
    </ol>

</li>

</ul>
</li>


<li>
<h5>Task B</h5>
<ul>
<li>
    <strong>Training and Validation Datasets:</strong> 
    
          Open domain HIREST dataset <a href="#ref3">[3]</a> to train the system
for Task B. HIREST comprises 3.4K text-video pairs sourced from an instructional video
dataset. Among these, 1.1K videos are annotated with moment spans pertinent to text
queries. Each moment is further dissected into key instructional steps, complete with captions
and timestamps, resulting in a total of 8.6K step captions.
    </ol>

</li>
</ul>
</li>




</ul>
</li>
</ul>

</section>


<section id="eval" style="margin-bottom: 1cm;">
<h2>Evaluation Metrics</h2>
<ul>
<li>
<h5>Task A</h5>
<ul>
<li>
   
   The VCVAL task consists of two sub-tasks: video retrieval (VR) and visual answer localization (VAL). We will evalaute the performance of the video retrieval system in terms of Mean Average Precision (MAP), Recall@k, Precision@k, and nDCG metrics with k={5, 10}. We will follow the <a href=https://github.com/usnistgov/trec_eval>trec_eval</a> evaluation library to report the performance of participating systems. For the VAL task, following MedVidQA 2023, we will use Mean Intersection over Union (mIoU) and IoU =0.3, IoU=0.5 and IoU=0.7 as the evaluation metrics. 

</li>

</ul>
</li>


<li>
<h5>Task B</h5>
<ul>
<li>
   
We plan to evaluate the performance of the step captioning task on two fronts: <b>(1)</b> how close the system-generated step caption is to the ground truth step captions, and <b>(2)</b> how well the predicted step segment aligns with the ground truth step segment. 
<br>

<ol>
    <br>
<li>We will measure the closeness in two ways:  

<ol type="a">

   <li> With the help of predicted timestamps and sentence-level similarity (ROUGE-L) of the step, we will align a predicted step to one of the ground truth steps. Once a predicted or ground truth step is matched, it is no longer considered, so there will be only one-to-one matching.  Towards this, we define the following:</li>
    <ul>
        <li> TP (True Positives): Represents the count of predicted steps that are present in the ground truth steps.</li>
  <li> FP (False Positives) :
 Represents the count of predicted steps that are not present in the ground truth steps. </li>

<li> FN (False Negatives): 
  Represents the count of ground truth steps that are not present in the predicted steps. </li>
</ul>
 Following this, we compute the precision, recall, and f-score. 
    <li>We will use the n-gram matching metrics: CIDEr <a href="#ref4">[4]</a>, and SPICE <a href="#ref5">[5]</a>. Additionally, we plan to use sentence-level embedding-based metrics, BERTScore <a href="#ref6">[6]</a> as it captures the semantic similarity between the generated and ground truth captions.  </li>

</ol>
</li>
<br>
<li>To compute the alignment between the predicted step segments and the ground truth step segments, we will use the intersection over union (IoU) metric. For a given step, IoU is computed as the ratio of the common segment to the union between the predicted and ground-truth segments. It ranges from 0 to 1. For the shorter step, where the step segment lasts, say, only 1-2 seconds, if the system-generated step segment does not match with the ground-truth segment, the system may end up with IoU=0. To deal with such a situation, we will use relaxed IoU, where we will extend the segments by &lambda; before computing the IoU. We will compute the mean of the IoU for all the segments in the test set.
</li>
</ol>


</li>
</ul>
</li>




</ul>
</li>
</ul>


</section>

<section id="organizers"></section>
<h2>Organizers</h2>
<p>
<a href="https://lhncbc.nlm.nih.gov/LHC-personnel/staff/DeepakGupta.html">
<div class="item">
    <img class="headshot" src="img/organizers/deepak.jpeg"/>
    <span class="name">Deepak Gupta</span>
    <span class="affiliation">NLM, NIH</span>
</div>
</a>
<a href="https://www.nlm.nih.gov/research/researchstaff/DemnerFushmanDina.html">
<div class="item">
    <img class="headshot" src="img/organizers/DinaDemnerFushman.jpg"/>
    <span class="name">Dina Demner-Fushman</span>
    <span class="affiliation">NLM, NIH</span>
</div>
</a>

</p>


<h2>References</h2>
<div id=ref1>[1] <i>Deepak Gupta, Kush Attal, and Dina Demner-Fushman. A Dataset for Medical Instructional Video Classification and Question Answering, Sci Data 10, 158 (2023) </i></div>
<div id=ref2>[2] <i>Zhong Ji, Yaru Ma, Yanwei Pang, and Xuelong Li. Query-aware sparse coding for web multi-
video summarization. Information Sciences, 478:152–166, 2019.</i></div>
<div id=ref3>[3] <i>Abhay Zala, Jaemin Cho, Satwik Kottur, Xilun Chen, Barlas Oguz, Yashar Mehdad, and
Mohit Bansal. Hierarchical video-moment retrieval and step-captioning. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 23056–23065,
2023.</i></div>

<div id=ref4>[4] <i>Vedantam, Ramakrishna, C. Lawrence Zitnick, and Devi Parikh. "Cider: Consensus-based image description evaluation." In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 4566-4575. 2015.</i></div>

<div id=ref5>[5] <i>Anderson, Peter, Basura Fernando, Mark Johnson, and Stephen Gould. "Spice: Semantic propositional image caption evaluation." In Computer Vision–ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part V 14, pp. 382-398. Springer International Publishing, 2016.</i></div>


<div id=ref6>[6] <i>Zhang, Tianyi, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, and Yoav Artzi. "Bertscore: Evaluating text generation with bert." International Conference on Learning Representations (2020).</i></div>

</main>

<footer>
<section>
<p>&copy; MedVidQA 2024 Organizers; Credit: <a href="https://www.cs.ubc.ca/research/image-matching-challenge/current/" style="color: black;">Image Matching Challenge 2021</a> </p>
<p>
    <a href="mailto: trec-medvidqa2024@googlegroups.com">E-mail</a>
</p>
</section>
</footer>

</body>
</html>
